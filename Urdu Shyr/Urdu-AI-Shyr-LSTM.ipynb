{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danial-amin/AI-Shyr/blob/main/Urdu%20Shyr/Urdu-AI-Shyr-LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thirty-hobby",
      "metadata": {
        "id": "thirty-hobby"
      },
      "source": [
        "## Urdu AI-Shyr\n",
        "### A comparative study of Uni-gram/Bi-gram/Tri-gram models for poetry generation in Urdu.\n",
        "#### Created by: Danial Amin\n",
        "##### <a href=\"https://danial-amin.github.io/#home\" target=\"_blank\">Reach me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "general-atmosphere",
      "metadata": {
        "id": "general-atmosphere",
        "outputId": "f16fa01f-5efb-41b4-a342-97ded22ed269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of Corpus Sentences:  2932\n",
            "Sample:  اقبال تیری عظمت کی داستاں کیا سناؤں\n"
          ]
        }
      ],
      "source": [
        "#Reading From Text Files\n",
        "file1 = open('iqbal.txt', 'r')\n",
        "LinesIqbal = file1.readlines()\n",
        "\n",
        "file2 = open('ghalib.txt', 'r')\n",
        "LinesGhalib = file2.readlines()\n",
        "\n",
        "file3 = open('faiz.txt', 'r')\n",
        "LinesFaiz = file3.readlines()\n",
        "\n",
        "corpus = []\n",
        "startingWords = []\n",
        "endingWords = []\n",
        " \n",
        "# Save the text in List\n",
        "for line in LinesIqbal:\n",
        "    if(line != '\\n'):\n",
        "        corpus.append(line.strip())\n",
        "        #Get Starting and Ending Words\n",
        "        word_list = line.split()\n",
        "        if(word_list):\n",
        "            startingWords.append(word_list[0])\n",
        "            endingWords.append(word_list[-1])\n",
        "        \n",
        "for line in LinesGhalib:\n",
        "    if(line != '\\n'):\n",
        "        corpus.append(line.strip())\n",
        "        #Get Starting and Ending Words\n",
        "        word_list = line.split()\n",
        "        if(word_list):\n",
        "            startingWords.append(word_list[0])\n",
        "            endingWords.append(word_list[-1])\n",
        "        \n",
        "for line in LinesFaiz:\n",
        "    if(line != '\\n'):\n",
        "        corpus.append(line.strip())\n",
        "        #Get Starting and Ending Words\n",
        "        word_list = line.split()\n",
        "        if(word_list):\n",
        "            startingWords.append(word_list[0])\n",
        "            endingWords.append(word_list[-1])\n",
        "\n",
        "# Testing\n",
        "print(\"Length of Corpus Sentences: \", len(corpus))\n",
        "print(\"Sample: \", corpus[532])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conservative-round",
      "metadata": {
        "id": "conservative-round"
      },
      "outputs": [],
      "source": [
        "#Importing Spacy library\n",
        "\n",
        "import spacy\n",
        "unlp = spacy.blank('ur')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "executive-external",
      "metadata": {
        "id": "executive-external"
      },
      "outputs": [],
      "source": [
        "# Adding special case in spacy for sentence start and end tags\n",
        "#from spacy.attrs import ORTH, NORM\n",
        "#case1 = [{ORTH: \"<s>\"}]\n",
        "#case2 = [{ORTH: \"<\\s>\"}]\n",
        "#unlp.tokenizer.add_special_case(\"<s>\", case1)\n",
        "#unlp.tokenizer.add_special_case(\"<\\s>\", case2)\n",
        "\n",
        "# Adding start and end tags in sentences\n",
        "#for index in range(len(corpus)):\n",
        "    #corpus[index] = \"<s> \" + corpus[index] + \" <\\s>\"\n",
        "\n",
        "# Testing\n",
        "#print(\"Length of Corpus Sentences: \", len(corpus))\n",
        "#print(\"Sample: \", corpus[532])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pleasant-chaos",
      "metadata": {
        "id": "pleasant-chaos",
        "outputId": "3d245ee9-cced-4a87-8be3-ffd6d141e368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of Corpus Words:  24841\n",
            "Sample:  آشیانہ\n"
          ]
        }
      ],
      "source": [
        "#Tokenizing corpus into words\n",
        "\n",
        "corpusTokens = []\n",
        "corpusWords = []\n",
        "\n",
        "#Word Tokenization\n",
        "for sentence in corpus:\n",
        "    words = unlp(sentence)\n",
        "    \n",
        "    for word in words:\n",
        "        corpusTokens.append(word)\n",
        "        \n",
        "for index in range(len(corpusTokens)):\n",
        "    corpusWords.append(corpusTokens[index].text)\n",
        "\n",
        "# Some Cleaning\n",
        "\"\"\"\n",
        "for index, element in enumerate(corpusWords):\n",
        "    if(element == '’'):\n",
        "        corpusWords[index] = -1\n",
        "        \n",
        "    if(element == '،'):\n",
        "        corpusWords[index] = -1\n",
        "    \n",
        "    if(element == '!'):\n",
        "        corpusWords[index] = -1\n",
        "        \n",
        "    if(element == '؟'):\n",
        "        corpusWords[index] = -1\n",
        "\n",
        "for index, element in enumerate(startingWords):\n",
        "    if(element == '’'):\n",
        "        startingWords[index] = -1\n",
        "        \n",
        "    if(element == '،'):\n",
        "        startingWords[index] = -1\n",
        "    \n",
        "    if(element == '!'):\n",
        "        startingWords[index] = -1\n",
        "        \n",
        "    if(element == '؟'):\n",
        "        startingWords[index] = -1\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "# Testing\n",
        "print(\"Length of Corpus Words: \", len(corpusWords))\n",
        "print(\"Sample: \", corpusWords[532])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lucky-bachelor",
      "metadata": {
        "id": "lucky-bachelor"
      },
      "source": [
        "# Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rational-officer",
      "metadata": {
        "id": "rational-officer"
      },
      "source": [
        "### UNI-GRAM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clinical-complaint",
      "metadata": {
        "id": "clinical-complaint",
        "outputId": "e7bbd0e6-ede3-4034-d83d-b5ef81520395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of UNIGRAM:  24841\n",
            "Sample:  ('آشیانہ', 0.00020128014170121977)\n"
          ]
        }
      ],
      "source": [
        "unigrams = []\n",
        "\n",
        "# Formula of UNIGRAM: P(wi) = count(wi) / count(total number of words)\n",
        "\n",
        "totalNumberOfWords = len(corpusWords)\n",
        "\n",
        "for word in corpusWords:\n",
        "    \n",
        "    if(word != -1):\n",
        "        probabilityOfWord = corpusWords.count(word) / totalNumberOfWords\n",
        "        unigrams.append((word,probabilityOfWord))\n",
        "    \n",
        "# Testing\n",
        "print(\"Length of UNIGRAM: \", len(unigrams))\n",
        "print(\"Sample: \", unigrams[532])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "forty-compilation",
      "metadata": {
        "id": "forty-compilation"
      },
      "source": [
        "### BI-GRAM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "demographic-dimension",
      "metadata": {
        "id": "demographic-dimension",
        "outputId": "6eedbf2d-c63a-427c-b0a0-4c70fcd6e69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of BIGRAM:  24840\n",
            "Sample:  ('اے', 'کبھی', 0.044444444444444446)\n"
          ]
        }
      ],
      "source": [
        "bigrams = []\n",
        "\n",
        "# Formula of BIGRAM: P(wi|wi-1) = count(wi-1 wi) / count(wi-1)\n",
        "\n",
        "for index in range(len(corpusWords)-1):\n",
        "    \n",
        "    word1 = corpusWords[index] #wi-1\n",
        "    word2 = corpusWords[index+1] #wi\n",
        "    \n",
        "    combinedWordCount = 0\n",
        "    \n",
        "    if(word1 != -1 and word2 != -1):\n",
        "    \n",
        "        for index in range(len(corpusWords)-1):\n",
        "            if(corpusWords[index] == word1 and corpusWords[index+1] == word2):\n",
        "                combinedWordCount += 1\n",
        "\n",
        "        probabilityOfWord = combinedWordCount / corpusWords.count(word1)\n",
        "        #Appending Tuple in form: (Probability of wi, given wi-1, CalculatedProb)\n",
        "        bigrams.append((word2,word1,probabilityOfWord))\n",
        "\n",
        "# Testing\n",
        "print(\"Length of BIGRAM: \", len(bigrams))\n",
        "print(\"Sample: \", bigrams[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "juvenile-signal",
      "metadata": {
        "id": "juvenile-signal"
      },
      "source": [
        "### Backward BI-GRAM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eligible-imperial",
      "metadata": {
        "id": "eligible-imperial",
        "outputId": "08a156d2-e461-4b66-f2d2-4735acd64eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of Backward BIGRAM:  24840\n",
            "Sample:  ('کبھی', 'اے', 0.029411764705882353)\n"
          ]
        }
      ],
      "source": [
        "backwardBigrams = []\n",
        "\n",
        "# Formula of backwardBIGRAM: P(wi-1|wi) = count(wi wi-1) / count(wi)\n",
        "\n",
        "for index in range(len(corpusWords)-1):\n",
        "    \n",
        "    word1 = corpusWords[index] #wi-1\n",
        "    word2 = corpusWords[index+1] #wi\n",
        "    \n",
        "    combinedWordCount = 0\n",
        "    \n",
        "    if(word1 != -1 and word2 != -1):\n",
        "    \n",
        "        for index in reversed(range(1,len(corpusWords))):\n",
        "            if(corpusWords[index] == word2 and corpusWords[index-1] == word1):\n",
        "                combinedWordCount += 1\n",
        "\n",
        "        probabilityOfWord = combinedWordCount / corpusWords.count(word2)\n",
        "        #Appending Tuple in form: (Probability of wi-1, given wi, CalculatedProb)\n",
        "        backwardBigrams.append((word1,word2,probabilityOfWord))\n",
        "\n",
        "# Testing\n",
        "print(\"Length of Backward BIGRAM: \", len(backwardBigrams))\n",
        "print(\"Sample: \", backwardBigrams[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regular-enlargement",
      "metadata": {
        "id": "regular-enlargement"
      },
      "source": [
        "### TRI-GRAM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "harmful-outside",
      "metadata": {
        "id": "harmful-outside",
        "outputId": "d61cdf3e-d1b8-4708-d241-ee03554f7d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of TRIGRAM:  24839\n",
            "Sample:  ('حقیقت', 'کبھی', 'اے', 0.5)\n"
          ]
        }
      ],
      "source": [
        "trigrams = []\n",
        "\n",
        "# Formula of TRIGRAM: P(wi|wi-2 wi-1) = count(wi-2 wi-1 wi) / count(wi-2 wi-1)\n",
        "\n",
        "for index in range(len(corpusWords)-2):\n",
        "    \n",
        "    word1 = corpusWords[index] #wi-2\n",
        "    word2 = corpusWords[index+1] #wi-1\n",
        "    word3 = corpusWords[index+2] #wi\n",
        "    \n",
        "    if(word1 != -1 and word2 != -1 and word3 != -1):\n",
        "    \n",
        "        combinedWordCount = 0\n",
        "        PrevWordsCount = 0\n",
        "\n",
        "        for index in range(len(corpusWords)-2):\n",
        "            if(corpusWords[index] == word1 and corpusWords[index+1] == word2 and corpusWords[index+2] == word3):\n",
        "                combinedWordCount += 1\n",
        "\n",
        "        for index in range(len(corpusWords)-1):\n",
        "            if(corpusWords[index] == word1 and corpusWords[index+1] == word2):\n",
        "                PrevWordsCount += 1\n",
        "\n",
        "        probabilityOfWord = combinedWordCount / PrevWordsCount\n",
        "        #Appending Tuple in form: (Probability of wi, given wi-2 wi-1, CalculatedProb)\n",
        "        trigrams.append((word3,word1,word2,probabilityOfWord))\n",
        "\n",
        "# Testing\n",
        "print(\"Length of TRIGRAM: \", len(trigrams))\n",
        "print(\"Sample: \", trigrams[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "labeled-dollar",
      "metadata": {
        "id": "labeled-dollar"
      },
      "source": [
        "### Saving and Loading Models (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cellular-theme",
      "metadata": {
        "id": "cellular-theme"
      },
      "outputs": [],
      "source": [
        "# Saving models as json to avoid computation for later use.\n",
        "\n",
        "import json\n",
        "\n",
        "with open('UnigramModel.json', 'w') as f:\n",
        "    json.dump(unigrams,f)\n",
        "    \n",
        "with open('BigramModel.json', 'w') as f:\n",
        "    json.dump(bigrams,f)\n",
        "    \n",
        "with open('BackwardBigramModel.json', 'w') as f:\n",
        "    json.dump(backwardBigrams,f)\n",
        "    \n",
        "with open('TrigramModel.json', 'w') as f:\n",
        "    json.dump(trigrams,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "improved-document",
      "metadata": {
        "id": "improved-document"
      },
      "outputs": [],
      "source": [
        "# Loading models for use.\n",
        "\n",
        "with open('UnigramModel.json') as f:\n",
        "    unigrams = [tuple(x) for x in json.load(f)]\n",
        "\n",
        "with open('BigramModel.json') as f:\n",
        "    bigrams = [tuple(x) for x in json.load(f)]\n",
        "    \n",
        "with open('BackwardBigramModel.json') as f:\n",
        "    backwardBigrams = [tuple(x) for x in json.load(f)]\n",
        "    \n",
        "with open('TrigramModel.json') as f:\n",
        "    trigrams = [tuple(x) for x in json.load(f)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "major-wound",
      "metadata": {
        "id": "major-wound"
      },
      "source": [
        "## Poetry Generation with Bi-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "original-needle",
      "metadata": {
        "id": "original-needle",
        "outputId": "90acad9b-244d-474a-d932-3ff8afecaca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "دل کا نہ ہوا ہے کہ ‘ تو کیا ہے کہ \n",
            "\n",
            "آہ و دل کا نہ ہوا ہے کہ ‘ تو کیا \n",
            "\n",
            "روانی ہاۓ موج ‘ تو کیا ہے کہ ‘ تو کیا \n",
            "\n",
            "لیکن اب اس کی ہے کہ ‘ تو کیا ہے کہ \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "نہ ہوا ہے کہ ‘ تو کیا ہے کہ ‘ تو \n",
            "\n",
            "دل کا نہ ہوا ہے کہ ‘ تو کیا ہے کہ \n",
            "\n",
            "تم نے کیا ہے کہ ‘ تو کیا ہے کہ ‘ \n",
            "\n",
            "پندار کا نہ ہوا ہے کہ ‘ تو کیا ہے کہ \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "کہ ‘ تو کیا ہے کہ ‘ تو کیا ہے کہ \n",
            "\n",
            "کاش اس کی ہے کہ ‘ تو کیا ہے کہ ‘ \n",
            "\n",
            "پیماں سے ‘ تو کیا ہے کہ ‘ تو کیا ہے \n",
            "\n",
            "توڑا جو تو کیا ہے کہ ‘ تو کیا ہے کہ \n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "    \n",
        "#poem = []\n",
        "inputsForOtherModels = []\n",
        "\n",
        "#For each of 3 stanzas\n",
        "for stanza in range(0,3):\n",
        "    #stanza = []\n",
        "    \n",
        "    #For each of 4 verses\n",
        "    for verse in range(0,4):\n",
        "        verse = []\n",
        "        \n",
        "        #Adding a starting Word\n",
        "        verse.append(random.choice(startingWords))\n",
        "        inputsForOtherModels.append(verse[0])\n",
        "        while verse[0] not in corpusWords:\n",
        "            verse[0] = random.choice(startingWords)\n",
        "            inputsForOtherModels.append(verse[0])\n",
        "        \n",
        "        print(verse[0], end = \" \")\n",
        "        \n",
        "        noOfWords = random.randint(7,10)\n",
        "        #For each for words in verse\n",
        "        for index in range(noOfWords):\n",
        "            candidateWords = []\n",
        "            candidateWordsProb = []\n",
        "    \n",
        "            for word in bigrams:\n",
        "                if(word[1] == verse[index]):\n",
        "                    candidateWords.append(word[0])\n",
        "                    candidateWordsProb.append(word[2])\n",
        "            \n",
        "            if candidateWordsProb:\n",
        "                maxProb = max(candidateWordsProb)\n",
        "                maxProbIndex = candidateWordsProb.index(maxProb)\n",
        "                \n",
        "                verse.append(candidateWords[maxProbIndex])\n",
        "                print(candidateWords[maxProbIndex], end = \" \")\n",
        "            else:\n",
        "                #wordTemp = random.choice(corpusWords)\n",
        "                #verse.append(wordTemp)\n",
        "                print(wordTemp, end = \" \")\n",
        "        \n",
        "        #stanza.append(verse)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    #poem.append(stanza)\n",
        "    #poem.append(\"\\n\")\n",
        "    print(\"\\n\\n\")\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "documented-television",
      "metadata": {
        "id": "documented-television"
      },
      "source": [
        "## Poetry Generation with Backward Bi-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "smooth-genre",
      "metadata": {
        "id": "smooth-genre",
        "outputId": "fed6b674-e585-44cf-b22c-cb23d1f46c9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "دل جلوں میں الجھ گیا کیونکر میسر میر سپاہ ناسزا لشکریاں \n",
            "\n",
            "آہ سحرگہی مجھ کو،کہ جہاں جادہ غیر از نمود کچھ کھٹکتا \n",
            "\n",
            "روانی ہاۓ صحبت مخالف ہے کھرا ہے کھرا ہے کھرا ہے \n",
            "\n",
            "لیکن عبث کہ چھپ کے پیتے تھے پیوست گلو بنا سکتے \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "نہ بیدار وہ سرود کیا دبدبۂ نادر کیا دبدبۂ نادر کیا \n",
            "\n",
            "دل جلوں میں الجھ گیا کیونکر میسر میر سپاہ ناسزا لشکریاں \n",
            "\n",
            "تم سبھی دوست ہونہیں سکتا غریب الدیار ہوں بجھا چاہتا ہوں \n",
            "\n",
            "پندار کا پيغام ہے کھرا ہے کھرا ہے کھرا ہے کھرا \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "کہ چھپ کے پیتے تھے پیوست گلو بنا سکتے غلاموں کی \n",
            "\n",
            "کاش رِضواں ہی خودکشی کرے اخذِ فیضِ جاں ستاں ناوک خیز \n",
            "\n",
            "پیماں سے باندھا گیا کیونکر میسر میر سپاہ ناسزا لشکریاں شکستہ \n",
            "\n",
            "توڑا جوشِ قدح پہ معشوق فریبی عنواں اٹھایئے گر لکھنے بیٹھوں \n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "    \n",
        "#poem = []\n",
        "inputsIndex = 0\n",
        "\n",
        "#For each of 3 stanzas\n",
        "for stanza in range(0,3):\n",
        "    #stanza = []\n",
        "    \n",
        "    #For each of 4 verses\n",
        "    for verse in range(0,4):\n",
        "        verse = []\n",
        "        \n",
        "        #Adding a starting Word\n",
        "        verse.append(inputsForOtherModels[inputsIndex])\n",
        "        while verse[0] not in corpusWords:\n",
        "            verse[0] = random.choice(startingWords)\n",
        "        inputsIndex += 1\n",
        "            \n",
        "        print(verse[0], end = \" \")\n",
        "        \n",
        "        noOfWords = random.randint(7,10)\n",
        "        #For each for words in verse\n",
        "        for index in range(noOfWords):\n",
        "            candidateWords = []\n",
        "            candidateWordsProb = []\n",
        "    \n",
        "            for word in backwardBigrams:\n",
        "                if(word[0] == verse[index]):\n",
        "                    candidateWords.append(word[1])\n",
        "                    candidateWordsProb.append(word[2])\n",
        "            \n",
        "            if candidateWordsProb:\n",
        "                maxProb = max(candidateWordsProb)\n",
        "                maxProbIndex = candidateWordsProb.index(maxProb)\n",
        "                \n",
        "                verse.append(candidateWords[maxProbIndex])\n",
        "                print(candidateWords[maxProbIndex], end = \" \")\n",
        "            else:\n",
        "                #wordTemp = random.choice(corpusWords)\n",
        "                #verse.append(wordTemp)\n",
        "                #print(wordTemp, end = \" \")\n",
        "                print(\"Word not Found!\")\n",
        "        \n",
        "        #stanza.append(verse)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    #poem.append(stanza)\n",
        "    #poem.append(\"\\n\")\n",
        "    print(\"\\n\\n\")\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "basic-opera",
      "metadata": {
        "id": "basic-opera"
      },
      "source": [
        "## Poetry Generation with Bidirectional Bi-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prospective-ceramic",
      "metadata": {
        "id": "prospective-ceramic",
        "outputId": "336c8fb9-e234-4221-c0b9-82437ccb8364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "دل جلوں میں الجھ کر تلف کھول کے پیتے تھے پیوست \n",
            "\n",
            "آہ سحرگہی مجھ کو،کہ جہاں جادہ غیر از نمود کچھ کھٹکتا \n",
            "\n",
            "روانی ہاۓ موج لرزاں ہے کھرا ہے کھرا ہے کھرا ہے \n",
            "\n",
            "لیکن عبث کہ چھپ کے پیتے تھے پیوست گلو بنا ہے \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "نہ بیدار وہ سرود کیا دبدبۂ نادر کیا دبدبۂ نادر کیا \n",
            "\n",
            "دل جلوں میں الجھ کر تلف کھول کے پیتے تھے پیوست \n",
            "\n",
            "تم سبھی کچھ کھٹکتا تھا شغف فقیری ملی انکو سرفرازی نازاں \n",
            "\n",
            "پندار کا پيغام ہے کھرا ہے کھرا ہے کھرا ہے کھرا \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "کہ چھپ کے پیتے تھے پیوست گلو بنا ہے کھرا ہے \n",
            "\n",
            "کاش رِضواں ہی خودکشی کرے اخذِ فیضِ جاں ستاں ناوک خیز \n",
            "\n",
            "پیماں سے باندھا گیا کیونکر میسر میر سپاہ ناسزا لشکریاں شکستہ \n",
            "\n",
            "توڑا جو چرا کر تلف کھول کے پیتے تھے پیوست گلو \n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "    \n",
        "#poem = []\n",
        "inputsIndex = 0\n",
        "\n",
        "#For each of 3 stanzas\n",
        "for stanza in range(0,3):\n",
        "    #stanza = []\n",
        "    \n",
        "    #For each of 4 verses\n",
        "    for verse in range(0,4):\n",
        "        verse = []\n",
        "        \n",
        "        #Adding a starting Word\n",
        "        verse.append(inputsForOtherModels[inputsIndex])\n",
        "        while verse[0] not in corpusWords:\n",
        "            verse[0] = random.choice(startingWords)\n",
        "        inputsIndex += 1\n",
        "            \n",
        "        print(verse[0], end = \" \")\n",
        "        \n",
        "        noOfWords = random.randint(7,10)\n",
        "        #For each for words in verse\n",
        "        for index in range(noOfWords):\n",
        "            candidateWords = []\n",
        "            candidateWordsProb = []\n",
        "            \n",
        "            for word in bigrams:\n",
        "                if(word[1] == verse[index]):\n",
        "                    candidateWords.append(word[0])\n",
        "                    candidateWordsProb.append(word[2])\n",
        "    \n",
        "            for word in backwardBigrams:\n",
        "                if(word[0] == verse[index]):\n",
        "                    candidateWords.append(word[1])\n",
        "                    candidateWordsProb.append(word[2])\n",
        "            \n",
        "            if candidateWordsProb:\n",
        "                maxProb = max(candidateWordsProb)\n",
        "                maxProbIndex = candidateWordsProb.index(maxProb)\n",
        "                \n",
        "                verse.append(candidateWords[maxProbIndex])\n",
        "                print(candidateWords[maxProbIndex], end = \" \")\n",
        "            else:\n",
        "                #wordTemp = random.choice(corpusWords)\n",
        "                #verse.append(wordTemp)\n",
        "                #print(wordTemp, end = \" \")\n",
        "                print(\"Word not Found!\")\n",
        "        \n",
        "        #stanza.append(verse)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    #poem.append(stanza)\n",
        "    #poem.append(\"\\n\")\n",
        "    print(\"\\n\\n\")\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "engaged-switch",
      "metadata": {
        "id": "engaged-switch"
      },
      "source": [
        "## Poetry Generation with Tri-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impaired-flesh",
      "metadata": {
        "id": "impaired-flesh",
        "outputId": "b9213272-fcbc-4f4d-d576-581588607ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "حسن میں رہیں ، کھائیں گے کیا ؟ رکھ دیا \n",
            "\n",
            "نفی سے کرتی ہے چمک جن کی ستاروں کو عرق \n",
            "\n",
            "اس دکھاوے سے دل جلوں میں شمار ہوگا جو \n",
            "\n",
            "سکوت پردۂ ساز میں تو پیر مے خانہ ہر \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "یہ کہا کہ وہ اثر کہن نہ تری حکایت \n",
            "\n",
            "رنجش دل ‘ یک جہاں ویراں کرے گی جو \n",
            "\n",
            "یہ کہا کہ وہ اثر کہن نہ تری حکایت \n",
            "\n",
            "علاج ضعف یقیں ان سے ہو ‘ تو غربت \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "غم مقامات آہ و فغاں اور بھی ہیں ابھی عشق کے \n",
            "\n",
            "مالی نے کسی اور شجر کے سجا دیا لکھی \n",
            "\n",
            "کہ ہزاروں سجدے تڑپ رہے ہیں وہ آنسو کرتی ہے چمک \n",
            "\n",
            "گرچہ تو زنداني اسباب ہے قلب کو ليکن \n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import random\n",
        "    \n",
        "#poem = []\n",
        "\n",
        "#For each of 3 stanzas\n",
        "for stanza in range(0,3):\n",
        "    #stanza = []\n",
        "    \n",
        "    #For each of 4 verses\n",
        "    for verse in range(0,4):\n",
        "        verse = []\n",
        "        \n",
        "        #Adding two starting Words\n",
        "        verse.append(random.choice(startingWords))\n",
        "        if verse[0] in corpusWords:\n",
        "            indexForNextWord = corpusWords.index(verse[0])\n",
        "            verse.append(corpusWords[indexForNextWord+1])\n",
        "        else:   \n",
        "            while verse[0] not in corpusWords:\n",
        "                verse[0] = random.choice(startingWords)\n",
        "\n",
        "            indexForNextWord = corpusWords.index(verse[0])\n",
        "            verse.append(corpusWords[indexForNextWord+1])\n",
        "        \n",
        "        print(verse[0], verse[1], end = \" \")\n",
        "        \n",
        "        noOfWords = random.randint(7,10)\n",
        "        #For each for words in verse\n",
        "        for index in range(1, noOfWords):\n",
        "            candidateWords = []\n",
        "            candidateWordsProb = []\n",
        "    \n",
        "            for word in trigrams:\n",
        "                if(word[1] == verse[index-1] and word[2] == verse[index]):\n",
        "                    candidateWords.append(word[0])\n",
        "                    candidateWordsProb.append(word[3])\n",
        "            \n",
        "            if candidateWordsProb:\n",
        "                maxProb = max(candidateWordsProb)\n",
        "                maxProbIndex = candidateWordsProb.index(maxProb)\n",
        "                \n",
        "                verse.append(candidateWords[maxProbIndex])\n",
        "                print(candidateWords[maxProbIndex], end = \" \")\n",
        "            else:\n",
        "                #wordTemp = random.choice(corpusWords)\n",
        "                #verse.append(wordTemp)\n",
        "                #print(wordTemp, end = \" \")\n",
        "                print(\"No Word Found!\")\n",
        "        \n",
        "        #stanza.append(verse)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    #poem.append(stanza)\n",
        "    #poem.append(\"\\n\")\n",
        "    print(\"\\n\\n\")\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fossil-brick",
      "metadata": {
        "id": "fossil-brick"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}